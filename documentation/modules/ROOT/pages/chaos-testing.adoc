:markup-in-source: verbatim,attributes,quotes
:CHE_URL: http://codeready-workspaces.%APPS_HOSTNAME_SUFFIX%
:USER_ID: %USER_ID%
:OPENSHIFT_PASSWORD: %OPENSHIFT_PASSWORD%
:OPENSHIFT_CONSOLE_URL: https://console-openshift-console.%APPS_HOSTNAME_SUFFIX%/topology/ns/chaos-engineering{USER_ID}/graph
:APPS_HOSTNAME_SUFFIX: %APPS_HOSTNAME_SUFFIX%
:KIALI_URL: https://kiali-istio-system.%APPS_HOSTNAME_SUFFIX%
:GRAFANA_URL: https://grafana-istio-system.%APPS_HOSTNAME_SUFFIX%

= Run Chaos testing

_XX MINUTE PRACTICE_

== Plan an Experiment

== What is OpenShift Service Mesh?
[sidebar]
--
**OpenShift Service Mesh** is also a service available on top of OpenShift.

As applications evolve into collections of decentralized services, managing communications and security between those services becomes more difficult. 

**Red Hat OpenShift Service Mesh** provides a uniform way to connect, manage, and observe microservices-based applications.
--

What’s in **Red Hat OpenShift Service Mesh** ?

[tabs]
====
Istio::
+
--
**Istio** is an open-source project for integrating and managing traffic flow across services. It works in concert with an underlying cluster manager (like Kubernetes). Centralized components, sidecar proxies, and node agents work together to create the data and control planes over a distributed application.
--
Tracing::
+
--
Tracing allows you to track a single request as it makes its way between different services - or even inside a service - providing insight into the entire request process from start to finish. OpenShift service mesh uses **Jaeger**, an open, distributed tracing system.
--
Visualization::
+
--
Visualization helps users see communication pathways between services, how they’re being managed, and how traffic is flowing in near-real time for easier management and troubleshooting. OpenShift service mesh uses **Kiali**, an open source project, to view configuration, monitor traffic, and analyze traces.
--
====

== Observability with Kiali

**Kiali** provides an interactive graph view of your namespace in real time, being able to display the interactions at several levels (applications, versions, workloads), with contextual information and charts on the selected graph node or edge.

`*Click on the 'Developer Observability' button below*`

[link={KIALI_URL}]
[window=_blank, align="center"]
[role='params-link']
image::developer-observability-button.png[Developer Observability - Button, 300]

Then, `*log in with OpenShift as user{USER_ID}/{OPENSHIFT_PASSWORD}'*`

image::kiali-login.png[Kiali- Log In,300]

In the **'Graph' view**, `*enter the following configuration*`:

.Graph Settings
[%header,cols=2*]
|===
|Parameter
|Value

|Namespace 
|**chaos-engineering{USER_ID}**

|Type Graph
|**Versioned app graph**

|Display
|**'Response Time'** checked

**'Traffic Animation'** checked

|Hide...
|**service*=svc.cluster.local**

|===

The outcome is a graph with all the services, connected by the requests going through them. 
You can see how the services interact with each other. 

image::kiali-graph.png[Kiali- Graph,900]

**TODO: EXPLAIN THE APPLICATION**

== Chaos Experiment 1: Injecting latencies

In production, it is very common to have delay services rather than down services.

_How does **Travels Service** behave when one of its dependant services experiment network latencies?_

In the {KIALI_URL}[Kiali Console^], from the **'Graph' view**, `*right-click on the 'Hotels' service and select 'Details'*`

image::kiali-right-click-hotel-service.png[Kiali - Right Click Service,400]

You will be redirected to the Service Details page. 

`*Click on the 'Actions' > 'Fault Injection'*`

image::kiali-add-fault-injection.png[Kiali - Add Fault Injection,900]

`*Add HTTP Delay by entering the following settings:*`

.HTTP Delay Settings
[%header,cols=3*]
|===
|Parameter
|Value
|Description

|Add HTTP Delay 
|**Enabled**
|

|Delay Percentage
|**30**
|

|Fixed Delayed
|**1s**
|

|===

image::kiali-configure-latency.png[Kiali - Configure Latency,300]

`*Click on the 'Create' button*`. 

**30% of the traffic of the 'Hotels' service has now 1 second of delay.** Now let's see the impact of the application.

In the {GRAFANA_URL}[Chaos Engineering Dashboard], you can see the result of the chaos experiment.
First, `*select the following variables on the dashboard*`:

.Dashboard Settings
[%header,cols=3*]
|===
|Parameter
|Value
|Description

|Namespace 
|**chaos-engineering{USER_ID}**
|

|Service
|**travels**
|

|===

image::grafana-chaos-selection.png[Grafana - Chaos Selection,400]

From the **'Service Overview'** or **'Request Duration'** panels, you can tell that the "Travels" service is particularly impacted when there is a network latency with one of its dependant services.

image::kiali-latency-fault-overview.png[Kiali - Latency Fault Overview,900]

image::kiali-latency-fault-details.png[Kiali - Latency Fault Details,900]

== Chaos Experiment 1: Solving detected issues

To minimize the latency impact, you can use the **timeout pattern** which is probably the most common resilience pattern for distributed systems.
The goal is to fail a request after a certain period of time to avoid code and resources locking when the service is waiting for a response that takes too long or that might never arrive.

In the {KIALI_URL}[Kiali Console^], from the **'Services' view**, `*click on the 'travels' > 'Actions' > 'Request Timeouts'*`

image::kiali-request-timeout-actions.png[Kiali - Request Timeout Actions,900]

`*Add HTTP Timeout by entering the following settings:*`

.HTTP Timeout Settings
[%header,cols=3*]
|===
|Parameter
|Value
|Description

|Add HTTP Timeout 
|**Enabled**
|

|Timeout
|**50ms**
|

|===

image::kiali-configure-timeout.png[Kiali - Configure Timeout,400]

In {GRAFANA_URL}[Grafana^], from the **Chaos Engineering Dashboard**, `*scroll down and see the latency metrics*`

After a while, you can see the impact of our configuration. Indeed, the latency on **Travels** service has reduced and
the latency metrics become green again.

image::grafana-timeout-details-1.png[Grafana - Timeout Details,900]

image::grafana-timeout-details-2.png[Grafana - Timeout Details,900]

The latency issues have been fixed but the timeout pattern introduces errors for the requests which exceeds the threshold.
`*Scroll up and see the error rate metrics*`

image::grafana-timeout-error.png[Grafana - Timeout Error,900]

== TODO

You have implemented timeouts for the travels service. 
Let's implementing a strategy of retry to mitigate these transient errors.

In the {KIALI_URL}[Kiali Console^], from the **'Services' view**, 
`*click on the 'travels' > 'Actions' > 'Request Timeouts' and add HTTP Retry by entering the following settings:*`

.HTTP Retry Settings
[%header,cols=3*]
|===
|Parameter
|Value
|Description

|Add HTTP Retry 
|**Enabled**
|

|Attempts
|**3**
|

|Per Try Timeout
|**10ms**
|

|===

image::kiali-configure-retry.png[Kiali - Configure Retry,400]

`*Back to {GRAFANA_URL}[Grafana^]*`, you can tell the retry pattern reduces the error rates without impacting the latency.

== TODO

