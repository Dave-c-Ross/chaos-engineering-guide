:markup-in-source: verbatim,attributes,quotes
:CHE_URL: http://codeready-workspaces.%APPS_HOSTNAME_SUFFIX%
:USER_ID: %USER_ID%
:OPENSHIFT_PASSWORD: %OPENSHIFT_PASSWORD%
:OPENSHIFT_CONSOLE_URL: https://console-openshift-console.%APPS_HOSTNAME_SUFFIX%/topology/ns/chaos-engineering{USER_ID}/graph
:APPS_HOSTNAME_SUFFIX: %APPS_HOSTNAME_SUFFIX%
:KIALI_URL: https://kiali-istio-system.%APPS_HOSTNAME_SUFFIX%
:GRAFANA_URL: https://grafana-istio-system.%APPS_HOSTNAME_SUFFIX%
:GITOPS_URL: https://argocd-server-argocd.%APPS_HOSTNAME_SUFFIX%

= Expérience Chaos 1: Latence du réseau

_30 MINUTE PRACTICE_

En production, il est plus fréquent d'avoir des services lents que des services rompus. **Latence** mesure le retard entre une action et une réponse. Pour cette première expérience, nous allons tester l'hypothèse suivante:

_**Une petite latence de réseau ne devrait pas affecter l'objectif de niveau de service (SLO) du Service de voyage**_

== Définir l'état stable


Dans le {GRAFANA_URL}[Chaos Engineering Dashboard, role='params-link'], vous pouvez analyser les différentes métriques et définir l'État Steady pour notre expérience de chaos.
Premièrement, `*select the following variables on the dashboard*`:

. Paramètres de tableau de bord
<MISSING TTKN12>
|MISUMI
|Paramètre
|Value
|MISUMI

|Namespace
|**chaos-engineering{USER_ID}**
|MISUMI|

|Service
|**travels.chaos-engineering{USER_ID}.svc.cluster.local**
|MISUMI|

|MISUMI

image::grafana-chaos-selection.png[Grafana - Chaos Selection,400]

De la section **Namespace**, vous pouvez dire que **99% des demandes sont réussies et servies dans les 50 ms**

image::grafana-steady-state-latency.png[Grafana - Steady State Latency,900]

Donc nous allons définir cette SLO comme "état ferme".

`*Click on 'Service Overview' > Edit*`

image::grafana-edit-service-overview.png[Grafana - Edit Service Overview,900]

Puis, `*click on 'Visualization Settings' icon on the left hand sidebar, scroll down to find the 'P99 Latency (Value #D)' rule and enter the following information for Thresholds*`

Réglages des seuils de latence P99
[%header,cols=3*]
|MISUMI
|Paramètre
|Value
|MISUMI

|Seuils
|**50,100**
|MISUMI|

|Mode de couleur
|**Cell**
|MISUMI|

|Colors
|**Green/Yellow/Red** (cliquez sur le bouton 'invert' si nécessaire)
|MISUMI|

|MISUMI

image::grafana-p99-latency-threholds.png[Grafana - P99 Latency Threholds,700]

`*Scroll down again and to find the 'Success Rate (Value #E)' rule and enter the following information for Thresholds*`

. Success Rate Thresholds Paramètres
[%header,cols=3*]
|MISUMI
|Paramètre
|Value
|MISUMI

|Seuils
***0,95,0,99**
|MISUMI|

|Mode de couleur
|**Cell**
|MISUMI|

|Colors
|Red/Yellow/Green** (cliquez sur le bouton 'invert' si nécessaire)
|MISUMI|

|MISUMI

image::grafana-success-rate-threholds.png[Grafana - Sucess Rate Threholds,700]

Une fois terminé, vous devriez avoir le résultat suivant (tout vert).

image::grafana-service-overview-configured.png[Grafana - Service Overview Configured,700]

`*Click on the 'Disk' icon to save and go back to the Dashboard.*`

== Exécutez l'expérience Chaos

Dans le {KIALI_URL}[Kiali Console^, role='params-link'], de la vue **'Graph'**, `*right-click on the 'discounts' service (triangle symbol) and select 'Details'*`

image::kiali-right-click-service.png[Kiali - Right Click Service,600]

Vous serez redirigé vers la page Détails du service.

`*Click on the 'Actions' > 'Fault Injection'*`

image::kiali-add-fault-injection.png[Kiali - Add Fault Injection,900]

`*Add HTTP Delay by entering the following settings:*`

. Paramètres de retard HTTP
[%header,cols=3*]
|MISUMI
|Paramètre
|Value
|MISUMI

|Add HTTP Delay
|MISUMI
|MISUMI|

∙ Pourcentage de retard
|**5**
|MISUMI|

|Fixed Delayed
|**1s**
|MISUMI|

|MISUMI

image::kiali-configure-latency.png[Kiali - Configure Latency,400]

`*Click on the 'Update' button*`.

**5% du trafic du service des « comptes » a maintenant 1 seconde de retard. **

== Analyser le résultat Chaos

Voyons maintenant l'impact de l'application.

Dans le {GRAFANA_URL}[Chaos Engineering Dashboard], vous pouvez voir le résultat de l'expérience de chaos.

image::grafana-latency-fault-overview.png[Grafana - Latency Fault Overview,900]

Depuis le panel **'Service Aperçu'** ou **'Request Durée'** pour le service 'voyages', vous pouvez dire ce qui suit sur la petite latence réseau basée sur notre hypothèse:

- there is no impact on the Success Rate of the overall requests (100%)
- there is a huge impact on the performance of the application. 

Indeed, just 1 second of delay on 5% of the traffic of one dependant service induces **a latency propagation of ~2 seconds across the entire system**.

TTKN1191

In conclusion, you can tell **the application is not resilient to a small network latency**. To reduce or fix this phenomenon, you could configure the autoscaling or implement a cache mechanism across the different services of the applications.

TTKN1225 Improve the Resiliency

To contain this latency propagation, you are going to apply the *Retry* pattern to all services calling the delayed 'discounts' services.

Retries can improve the application resiliency against transcient problems such as  a temporarily overloaded service or network like we simulate in our experiment.

Instead of failing directly or waiting too long, we could retry N number of times to get the desired output with the desired response time before considering as failed.

TTKN1239

TTKN1216
TTKN1226
cars::
+
--
In the TTKN1205[Kiali Console^, role='params-link'], from the **'Services' view**, TTKN1240

TTKN1241

.HTTP Retry Settings
TTKN1217
|===
|Parameter
|Value
|Description

|Add HTTP Retry 
|**Enabled**
|

|Attempts
|**5**
|

|Per Try Timeout
|**20ms**
|

|===

TTKN1192

TTKN1242.
--

flights::
+
--
In the TTKN1206[Kiali Console^, role='params-link'], from the **'Services' view**, TTKN1243

TTKN1244

.HTTP Retry Settings
TTKN1218
|===
|Parameter
|Value
|Description

|Add HTTP Retry 
|**Enabled**
|

|Attempts
|**5**
|

|Per Try Timeout
|**20ms**
|

|===

TTKN1193

TTKN1245.
--

hotels::
+
--
In the TTKN1207[Kiali Console^, role='params-link'], from the **'Services' view**, TTKN1246

TTKN1247

.HTTP Retry Settings
TTKN1219
|===
|Parameter
|Value
|Description

|Add HTTP Retry 
|**Enabled**
|

|Attempts
|**5**
|

|Per Try Timeout
|**20ms**
|

|===

TTKN1194

TTKN1248.
--

insurances::
+
--
Dans le {KIALI_URL}[Kiali Console^, role='params-link'], de la vue **'Services**, `*click on the 'insurances' service > 'Actions' > 'Request Timeouts'*`

`*Add HTTP Retry by entering the following settings:*`

. Réglages de rentrée HTTP
[%header,cols=3*]
|MISUMI
|Paramètre
|Value
|MISUMI

|Add HTTP Retry
|MISUMI
|MISUMI|

|Attempts
|**5**
|MISUMI|

|Per Try Timeout
|**20ms**
|MISUMI|

|MISUMI

image::kiali-configure-latency-retry.png[Kiali - Configure Latency Retry,400]

`*Click on the 'Update' button*`.
--
====

== Valider l'amélioration

Retour dans le {GRAFANA_URL}[Chaos Engineering Dashboard], vous pouvez dire que nous parvenons à contenir la propagation de latence de **ne dépassant pas 100 ms en général** en utilisant le modèle Retry tandis que le service 'discounts' a toujours le problème de latence 1s.

image::grafana-latency-contained-overview.png[Grafana - Latency Contained Overview,900]

Vous pouvez voir plus de détails sur le panneau 'Request Durée' pour le service 'voyages'

image::grafana-latency-contained-details.png[Grafana - Latency Contained Details,900]

== Retourner l'expérience Chaos

Il n'y a rien de plus simple que de retourner toutes les configurations que vous avez faites pendant ce laboratoire avec Argo CD.

Dans {GITOPS_URL}[Argo CD^, role='params-link'], `*click on 'Sync > Synchronize'*`.

image::argocd-rollback-sync.png[Argo CD - Sync Application, 900]

Enfin, dans le {GRAFANA_URL}[Chaos Engineering Dashboard], `*please check the application is back in the steady state*`.

image::grafana-steady-state.png[Grafana - Steady State,700]